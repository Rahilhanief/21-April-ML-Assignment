{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6624176f-efbd-4e5f-b204-34336f275001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEuclidean distance is the shortest path between source and destination which is a straight line \\nbut Manhattan distance is sum of all the real distances between source(s) and destination(d) and each\\ndistance are always the straight lines \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "Euclidean distance is the shortest path between source and destination which is a straight line \n",
    "but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each\n",
    "distance are always the straight lines \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bc1894-9edb-4363-b784-a92d83fe6063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe choice of k will largely depend on the input data as data with more outliers or noise will \\nlikely perform better with higher values of k. Overall, it is recommended to have an odd number for k \\nto avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"\n",
    "The choice of k will largely depend on the input data as data with more outliers or noise will \n",
    "likely perform better with higher values of k. Overall, it is recommended to have an odd number for k \n",
    "to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d366a6d-31b4-4695-b508-78a670e74a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe mean by the 'best distance metric' is the one that allows the KNN to classify test examples with \\nthe highest precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy.\\n\\nEuclidean distance calculates the distance between two real-valued vectors.\\nYou are most likely to use Euclidean distance when calculating the distance between two rows\\nof data that have numerical values, such a floating point or integer values.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "We mean by the 'best distance metric' is the one that allows the KNN to classify test examples with \n",
    "the highest precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy.\n",
    "\n",
    "Euclidean distance calculates the distance between two real-valued vectors.\n",
    "You are most likely to use Euclidean distance when calculating the distance between two rows\n",
    "of data that have numerical values, such a floating point or integer values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c943aa9e-bb5f-4448-bf52-af695905b678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe most important hyperparameter for KNN is the number of neighbors (n_neighbors).\\n\\nTest values between at least 1 and 21, perhaps just the odd numbers.\\n\\nn_neighbors in [1 to 21]\\nIt may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\\n\\nmetric in [‘euclidean’, ‘manhattan’, ‘minkowski’]\\n\\nIt may also be interesting to test the contribution of members of the neighborhood via different weightings (weights).\\n\\nweights in [‘uniform’, ‘distance’]\\n\\n#List Hyperparameters that we want to tune.\\nleaf_size = list(range(1,50))\\nn_neighbors = list(range(1,30))\\np=[1,2]\\n#Convert to dictionary\\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\\n#Create new KNN object\\nknn_2 = KNeighborsClassifier()\\n#Use GridSearch\\nclf = GridSearchCV(knn_2, hyperparameters, cv=10)\\n#Fit the model\\nbest_model = clf.fit(x,y)\\n#Print The value of best Hyperparameters\\nprint('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\\nprint('Best p:', best_model.best_estimator_.get_params()['p'])\\nprint('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors']\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"\n",
    "The most important hyperparameter for KNN is the number of neighbors (n_neighbors).\n",
    "\n",
    "Test values between at least 1 and 21, perhaps just the odd numbers.\n",
    "\n",
    "n_neighbors in [1 to 21]\n",
    "It may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\n",
    "\n",
    "metric in [‘euclidean’, ‘manhattan’, ‘minkowski’]\n",
    "\n",
    "It may also be interesting to test the contribution of members of the neighborhood via different weightings (weights).\n",
    "\n",
    "weights in [‘uniform’, ‘distance’]\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "#Fit the model\n",
    "best_model = clf.fit(x,y)\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5b1de-91bd-45f0-885b-c4effa0351dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"\n",
    "So if dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm. \n",
    "KNN is also very sensitive to noise in the dataset. If the dataset is large, there are chances of noise in\n",
    "the dataset which adversely affect the performance of KNN algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc6e17a-74dd-44b6-8a95-10a0d2f30943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSome Drawbacks of KNN :\\nAccuracy depends on the quality of the data.\\nWith large data, the prediction stage might be slow.\\nSensitive to the scale of the data and irrelevant features.\\nRequire high memory – need to store all of the training data.\\nGiven that it stores all of the training, it can be computationally expensive.\\n\\nOne way to do this is training KNN for different values of K and capturing the performance metrics for that value at K.\\nSay for eg. You consider accuracy as the performance measure then we plot a graph of accuracy vs. different values\\nof K and we select a point where we get the best accuracy.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "\"\"\"\n",
    "Some Drawbacks of KNN :\n",
    "Accuracy depends on the quality of the data.\n",
    "With large data, the prediction stage might be slow.\n",
    "Sensitive to the scale of the data and irrelevant features.\n",
    "Require high memory – need to store all of the training data.\n",
    "Given that it stores all of the training, it can be computationally expensive.\n",
    "\n",
    "One way to do this is training KNN for different values of K and capturing the performance metrics for that value at K.\n",
    "Say for eg. You consider accuracy as the performance measure then we plot a graph of accuracy vs. different values\n",
    "of K and we select a point where we get the best accuracy.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
